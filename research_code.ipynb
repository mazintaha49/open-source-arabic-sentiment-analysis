{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "research_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPV+zmhQANOMtd0U3t9e0rq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mazintaha49/open-source-arabic-sentiment-analysis/blob/main/research_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIpHJIw_9Fwi",
        "outputId": "f3e2a6a4-3460-4385-e3a0-68b6fb839dcc"
      },
      "source": [
        "%pip install --upgrade gupload\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user\n",
        "\n",
        "# update pandas, data frame (rows, colomns) processing library\n",
        "%pip install --upgrade pandas\n",
        "# update nltk, (NLP libarary)\n",
        "%pip install nltk --upgrade\n",
        "# install pyarabic (arabic text processing library)\n",
        "%pip install pyarabic\n",
        "# install Arabic-Stopwords, (the list of stopwords to be removed)\n",
        "%pip install Arabic-Stopwords\n",
        "\n",
        "\n",
        "# restart runtime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gupload in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-api-python-client==1.7.10 in /usr/local/lib/python3.7/dist-packages (from gupload) (1.7.10)\n",
            "Requirement already satisfied, skipping upgrade: click==7.0 in /usr/local/lib/python3.7/dist-packages (from gupload) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.7.10->gupload) (0.0.4)\n",
            "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.7.10->gupload) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.7.10->gupload) (0.17.4)\n",
            "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.7.10->gupload) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.7.10->gupload) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.7.10->gupload) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.7.10->gupload) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.7.10->gupload) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.7.10->gupload) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.7.10->gupload) (0.4.8)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.7/dist-packages (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.7/dist-packages (0.6.10)\n",
            "Requirement already satisfied: Arabic-Stopwords in /usr/local/lib/python3.7/dist-packages (0.3)\n",
            "Requirement already satisfied: pyarabic>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from Arabic-Stopwords) (0.6.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4f6qox2hNq"
      },
      "source": [
        "##Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvlX8tBa9SW8",
        "outputId": "e45e3c62-899a-4df6-eb08-e281293fc153"
      },
      "source": [
        "# loading dependencies (i.e. importing)\n",
        "\n",
        "\n",
        "# nltk, (multi NLP tasks related  model)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# pandas, data frame (rows, colomns) processing model\n",
        "import pandas as pd\n",
        "# numpy, numeric arrays and matrices (rows, colomns) processing model\n",
        "import numpy as np\n",
        "# re, provides regular expression matching operations\n",
        "import re\n",
        "# string, Common string operations\n",
        "import string\n",
        "# TweetTokenizer, a tokenizer form NLTK model\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "# tokenizer, defining the specifications of the tokenizer, and assigning a name to this specifications\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "# Arabic-Stopwords, (the list of stopwords to be removed)\n",
        "import arabicstopwords.arabicstopwords as stp\n",
        "# stemmer, a arabic stemmer from snowballstemmer model\n",
        "from snowballstemmer import stemmer\n",
        "stemmer_arb = stemmer(\"arabic\")\n",
        "# pyarabic (arabic text processing library)\n",
        "from pyarabic import araby"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivs14qjY2ueG"
      },
      "source": [
        "## Load Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHJh5Nz3AV8K",
        "outputId": "d827d3f1-d869-4207-c4a5-b61cded39d76"
      },
      "source": [
        "# defining a list of the stopwords\n",
        "stp_wd_lis = list(stp.stopwords_list())\n",
        "# printing the length of the list, i.e. stopwords count\n",
        "print(\"stopwords count \\n\", len(stp_wd_lis))\n",
        "# printing the first 5 elements\n",
        "print(\"first 5 elements \\n\", stp_wd_lis[:5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stopwords count \n",
            " 13629\n",
            "first 5 elements \n",
            " ['وعداهم', 'جنبنا', 'فباللتين', 'وبماذا', 'وأخونا']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i485ut-O23zR"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zs1CHd_ygN0",
        "outputId": "ac9b010e-fce4-402b-d40e-a3153b33a0ee"
      },
      "source": [
        "# load dataset\n",
        "path= 'https://raw.githubusercontent.com/mazintaha49/open-source-arabic-sentiment-analysis/main/dataset.csv'\n",
        "df_text_label= pd.read_csv(path, encoding= 'utf-8-sig')\n",
        "\n",
        "# printing dataset dataframe sample\n",
        "print(df_text_label.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                          tweet_text  label\n",
            "0                               قرفت انتحر عن الدرج    -1.0\n",
            "1                      صدقت يا رب إجعلنا راضين بقدرك    1.0\n",
            "2                            قرف شوهاد شي بخزي عنجد    -1.0\n",
            "3  صحح الخبر . مش تحرير المشتقات النفطية و إنما ت...   -1.0\n",
            "4  قصة جميلة جدا تعكس معنى الايمان و التمسك بالعق...    1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M2JH2ue2_DG"
      },
      "source": [
        "## Process Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTUG8M4szBkh",
        "outputId": "29207276-623d-40fc-ab93-52393a49c665"
      },
      "source": [
        "# view dataset info\n",
        "df_text_label.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1991 entries, 0 to 1990\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   tweet_text  1986 non-null   object \n",
            " 1   label       1980 non-null   float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 31.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "YQJofAM317GJ",
        "outputId": "39550bd4-16d2-4b15-a755-273737ac9c00"
      },
      "source": [
        "# view dataset describtive info\n",
        "df_text_label.describe(include= 'all')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1986</td>\n",
              "      <td>1980.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1966</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>للأسف البعض يعتقد ان المفاعل النووي سيحل مشاكل...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet_text        label\n",
              "count                                                1986  1980.000000\n",
              "unique                                               1966          NaN\n",
              "top     للأسف البعض يعتقد ان المفاعل النووي سيحل مشاكل...          NaN\n",
              "freq                                                    4          NaN\n",
              "mean                                                  NaN     0.000000\n",
              "std                                                   NaN     1.000253\n",
              "min                                                   NaN    -1.000000\n",
              "25%                                                   NaN    -1.000000\n",
              "50%                                                   NaN     0.000000\n",
              "75%                                                   NaN     1.000000\n",
              "max                                                   NaN     1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5Ugw23z2F2i",
        "outputId": "6ce358ac-ad73-4b64-95b8-c6db14f4a9c0"
      },
      "source": [
        "# search for null values within tweet_text column\n",
        "print(df_text_label.tweet_text.isnull().value_counts())\n",
        "\n",
        "# search for null values within label column\n",
        "print(df_text_label.label.isnull().value_counts())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False    1986\n",
            "True        5\n",
            "Name: tweet_text, dtype: int64\n",
            "False    1980\n",
            "True       11\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBJLponE3EIl"
      },
      "source": [
        "# Drop the null values \n",
        "df_text_label.dropna(inplace=True)\n",
        "# Reset index after drop\n",
        "df_text_label.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B8ntEhr31xD",
        "outputId": "a68227e6-327f-4f95-b695-71e7c0605be5"
      },
      "source": [
        "# confirm drop of null values\n",
        "\n",
        "# search for null values within tweet_text column\n",
        "print(df_text_label.tweet_text.isnull().value_counts())\n",
        "\n",
        "# search for null values within label column\n",
        "print(df_text_label.label.isnull().value_counts())\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False    1975\n",
            "Name: tweet_text, dtype: int64\n",
            "False    1975\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKHgZQvq4ES8",
        "outputId": "8c724568-e620-428a-df0f-43a26dabdb98"
      },
      "source": [
        "# check for duplicate values within tweet_text column\n",
        "df_text_label.tweet_text.duplicated().value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    1955\n",
              "True       20\n",
              "Name: tweet_text, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbNW2Lh64pmD"
      },
      "source": [
        "# drop duplicate values fro tweet_text column\n",
        "df_text_label.drop_duplicates(subset=['tweet_text'], inplace= True)\n",
        "# Reset index after drop\n",
        "df_text_label.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubXzzIwk5OyB",
        "outputId": "8d3e3a4d-473b-47e6-ce2e-32087fad4f1d"
      },
      "source": [
        "# confirm drop of duplicate values\n",
        "\n",
        "# check for duplicate values within tweet_text column\n",
        "df_text_label.tweet_text.duplicated().value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    1955\n",
              "Name: tweet_text, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "BElW0L8k5f2p",
        "outputId": "b8038984-4612-4ac6-a604-7539b81537e6"
      },
      "source": [
        "# view dataset describtive info\n",
        "df_text_label.describe(include= 'all')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1955</td>\n",
              "      <td>1955.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1955</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>ما اثقل دمك و ما اغباك</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    tweet_text        label\n",
              "count                     1955  1955.000000\n",
              "unique                    1955          NaN\n",
              "top     ما اثقل دمك و ما اغباك          NaN\n",
              "freq                         1          NaN\n",
              "mean                       NaN     0.009719\n",
              "std                        NaN     1.000209\n",
              "min                        NaN    -1.000000\n",
              "25%                        NaN    -1.000000\n",
              "50%                        NaN     1.000000\n",
              "75%                        NaN     1.000000\n",
              "max                        NaN     1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuhd-TXh8VAC"
      },
      "source": [
        "## Tweets Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDp6LpWK8PxK"
      },
      "source": [
        "# tweet processing function\n",
        "\n",
        "def tweet_preprocess(tweet):\n",
        "\n",
        "    # lower letter\n",
        "    tweet = tweet.lower()\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = ''.join(re.sub(r'^RT[\\s]+', \"twtinrt\", tweet).split())\n",
        "    # remove urls\n",
        "    tweet = ''.join(re.sub(\"(http\\S*)|(www\\S*)\", \"twtinurl\", tweet).split())\n",
        "    # remove only hash # sign from the word\n",
        "    tweet = ''.join(re.sub(r'#', \"twtinhash\", tweet))\n",
        "    # remove mention\n",
        "    tweet = ''.join(re.sub(\"(@[A-Za-z0-9]+)|(منشن\\S*)\", \"twtinment\", tweet).split())\n",
        "    # remove all digit form from word\n",
        "    tweet = str(''.join(word for word in tweet for char in word if char not in string.digits).split())\n",
        "    # tokenize tweet\n",
        "    tweet_token = tokenizer.tokenize(tweet)\n",
        "    # remove duplicated letters\n",
        "    tweet_dedup = []\n",
        "    for word in tweet_token:\n",
        "        word_dedup = \" \"\n",
        "        word_str = str(word)\n",
        "        word_dedup = ''.join(word_str[0])\n",
        "        for i in range (len(word_str)):\n",
        "            if i < (len(word_str) - 1):\n",
        "                init_char = word_str[i]\n",
        "                next_char = word_str[(i+1)]\n",
        "                if next_char != init_char:\n",
        "                    word_dedup += ''.join(next_char)\n",
        "        tweet_dedup.append(word_dedup)\n",
        "    # remove \"tashkeel\", \"tatweel\", and normal \"hamza\"\n",
        "    word_strip = []\n",
        "    for word in tweet_dedup:\n",
        "      word = araby.strip_tashkeel(word)\n",
        "      word = araby.strip_tatweel(word)\n",
        "      word = araby.normalize_hamza(word, method=\"tasheel\")\n",
        "      word_strip.append(word)\n",
        "    # remove punctuation, stopword, and word stemming\n",
        "    tweet_processed = []\n",
        "    for word in word_strip:\n",
        "      # remove punctuation, stopword\n",
        "      if word not in stp_wd_lis and word not in string.punctuation:\n",
        "          # word stemming\n",
        "          tweet_processed.append(stemmer_arb.stemWord(word))\n",
        "\n",
        "    return tweet_processed"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrVhOZ2kBj_Z"
      },
      "source": [
        "## Count word association"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es5WaAd28bil"
      },
      "source": [
        "# word association function\n",
        "\n",
        "def word_association(tweets, labels):\n",
        "    # naming the dictionary to hold association values (i.e. frequencies)\n",
        "    association_dictionary = {}\n",
        "    # defining a set to hold all unique words\n",
        "    uniq_wd_set= set()\n",
        "    for tweet, label in zip(tweets, labels):\n",
        "        for word in tweet_preprocess(tweet):\n",
        "          # adding only words with two or more letters, and less than seven letters\n",
        "          if len(word) <= 7 and len(word) >= 2:\n",
        "            # adding word to set (if unique, sets do not accept duplicated values)\n",
        "            uniq_wd_set.add(word)\n",
        "            # defining pair (e.g. (\"جيد\": 1))\n",
        "            pair = (word, label)\n",
        "            if pair in association_dictionary:\n",
        "                # increase count by 1\n",
        "                association_dictionary[pair] += 1\n",
        "            else:\n",
        "                # assign 1 as the association count \n",
        "                association_dictionary[pair] = 1\n",
        "\n",
        "    return association_dictionary, uniq_wd_set"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_hw0FsQDQxU"
      },
      "source": [
        "# building the word association dictionary\n",
        "wd_assoc_dict, uniq_wd_set = word_association(df_text_label.tweet_text, df_text_label.label)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW1HzeyOD_-b",
        "outputId": "acc1fdf4-88d2-4ca8-8b8c-497633919027"
      },
      "source": [
        "# view sample of the dictionary keys and values\n",
        "print(list(wd_assoc_dict.items())[:5])\n",
        "# view sample of the unique word set\n",
        "print(list(uniq_wd_set)[:5])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('صحالخبر', -1.0), 1), ((':)', 1.0), 1), (('مرعب', -1.0), 1), (('اعجب', -1.0), 1), (('هبل', -1.0), 3)]\n",
            "['حلو', 'مسكين', 'حمدله', 'خجلرجف', 'كساخ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prVNr6ARDOVO"
      },
      "source": [
        "## Perform count refinement (ratio + log)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_kLAr_RDFz6"
      },
      "source": [
        "# count refinement function\n",
        "\n",
        "def count_refinement(wd_assoc_dict, uniq_wd_set):\n",
        "  # define the new dectionary with refined values\n",
        "  refin_dict = {}\n",
        "  # get word count\n",
        "  for word in (uniq_wd_set):\n",
        "    # positive count\n",
        "    PN = wd_assoc_dict.get((word, 1.0), 1)\n",
        "    # negative count\n",
        "    NN = wd_assoc_dict.get((word, -1.0), 1)\n",
        "    # total count\n",
        "    TN = PN + NN\n",
        "    # Bayes Probability of word (PW)\n",
        "    PW_p = PN / TN\n",
        "    PW_n = 1 - PW_p     # Probability must add to 1\n",
        "    # Word Ratio (WR), first refinement\n",
        "    WR = PW_p / PW_n\n",
        "    # Log Word Ratio (LWR), second refinement\n",
        "    LWR = np.log(WR)\n",
        "    # update pair value with refined value\n",
        "    refin_dict[word] = LWR\n",
        "\n",
        "  return refin_dict"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qIk2XAKnmz4"
      },
      "source": [
        "# building the refined dictionary\n",
        "refin_dict = count_refinement(wd_assoc_dict, uniq_wd_set)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epU2W3nQhaHE",
        "outputId": "46f54a17-8572-4d43-c989-a22c6ff7dc82"
      },
      "source": [
        "# view sample of the refined dictionary\n",
        "print(list(refin_dict.items())[:5])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('حلو', 0.0), ('مسكين', 0.0), ('حمدله', 0.0), ('خجلرجف', 0.0), ('كساخ', 0.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n7LGNhipeRK"
      },
      "source": [
        "## Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7obyHSBoe4z"
      },
      "source": [
        "# making predictions function\n",
        "\n",
        "def make_prediction(tweets, refin_dict):\n",
        "\n",
        "  # define the prediction list\n",
        "  pred_lis = []\n",
        "  for tweet in tweets:\n",
        "    # define the LWR sum value holder\n",
        "    LWR_sum = 0\n",
        "    for word in tweet_preprocess(tweet):\n",
        "      # get word LWR\n",
        "      wd_LWR = refin_dict.get(word, 0)\n",
        "      # add word LWR to LWR_sum (for tweet LWR)\n",
        "      LWR_sum += wd_LWR\n",
        "      # make prediction\n",
        "      if LWR_sum >= 0:\n",
        "        pred_lis.append(1.0)\n",
        "      else:\n",
        "        pred_lis.append(-1.0)\n",
        "\n",
        "  return pred_lis\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbPHfiVtsNV4"
      },
      "source": [
        "pred_lis = make_prediction(df_text_label.tweet_text, refin_dict)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngk72gSusXNu",
        "outputId": "80795e69-aab7-4650-bb9d-df85ec948409"
      },
      "source": [
        "# view sample of the prediction list\n",
        "print(pred_lis[:5])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ_RrblTshz6"
      },
      "source": [
        "## Calculate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a91fpXBtsf4Q"
      },
      "source": [
        "# calculate accuracy function\n",
        "\n",
        "def calc_acc(labels, pred_lis):\n",
        "\n",
        "  # define error counter\n",
        "  error = 0\n",
        "  for label, pred_label in zip(labels, pred_lis):\n",
        "    print(label, pred_label)\n",
        "    # check for un-correct prediction\n",
        "    if label != pred_label:\n",
        "      error += 1\n",
        "  \n",
        "  # calculate accuracy\n",
        "  acc = 1 - (error / (len(labels)))\n",
        "\n",
        "  return acc"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPSobqTasmC0",
        "outputId": "c131770d-d7de-43dd-cb32-a009d9090d30"
      },
      "source": [
        "acc = calc_acc(df_text_label.label[:10], pred_lis[:10])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1.0 1.0\n",
            "1.0 1.0\n",
            "-1.0 1.0\n",
            "-1.0 1.0\n",
            "1.0 1.0\n",
            "1.0 1.0\n",
            "1.0 1.0\n",
            "-1.0 1.0\n",
            "1.0 -1.0\n",
            "1.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqh94t30uJkD"
      },
      "source": [
        "## Display results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXximOijuLEl",
        "outputId": "6def4622-16f6-492d-c4e2-d106d52032e6"
      },
      "source": [
        "print(\"Naive Bayes accuracy \\n\", acc)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy \n",
            " 0.512531969309463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YQTDLhfvSLf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}